{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model ELSA with NER and TSA\n",
    "- Create a dataset with only PER and ORG targets\n",
    "- Use the NoReCfine dev and train data not in the 50 docs\n",
    "- Create a training set with only NER targets as targets\n",
    "- Train TSA model with these data\n",
    "- Do inference on the 50 docs\n",
    "- Resolve entities as before\n",
    "- Count ELSA entity-level performance\n",
    "\n",
    "\n",
    "Obsidian: `exp_elsa-modelling-from-tsa.md`\n",
    "Conda: `transform`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the conll-formatted NoReC-fine TSA dataset\n",
    "- Join train and dev\n",
    "- split out the 50 documents  \n",
    "\n",
    "We have annotated 50 documents for sentiment towards each volitional entity. Due to its exploratory character have we taken these data from the train split. That may not have been the best decision, but we mitigate that by using the rest of train and dev data for training. We do not touch the official test split for now, in case it will be important later, that these data were not seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "from helpers import *\n",
    "from transformers import  pipeline\n",
    "from tqdm import tqdm\n",
    "from seqeval.metrics import classification_report\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "import datasets\n",
    "import numpy as np\n",
    "from datasets import ClassLabel, Sequence, Value, load_dataset, load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "conll_folder = \"norec_tsa/tsa_conll\"\n",
    "elsa_target_folder = \"data/elsa_conll\"\n",
    "cache_path = \"data/elsa_cache.json\" # Delete that file to recreate the data\n",
    "elsa_ds_folder = \"data/ds_elsa\"\n",
    "# elsapol_folder = \"data/ds_elsapol\"\n",
    "for f in [conll_folder, elsa_target_folder ,elsa_ds_folder ]:\n",
    "    if not os.path.exists(f):\n",
    "        os.mkdir(f)\n",
    "\n",
    "elsa_test = [\"300040\", \"107011\", \"201849\", \"301323\", \"106679\", \"109778\", \"004340\", \"102785\", \"105949\", \"109227\", \"101882\", \"601171\", \"107972\", \"103164\", \"003939\", \"702913\", \"201734\", \"300178\", \"003717\", \"600774\", \"500437\", \"704907\", \"200937\", \"109021\", \"501037\", \"302181\", \"702152\", \"103447\", \"702956\", \"100866\", \"202792\", \"602054\", \"302663\", \"201470\", \"004230\", \"110613\", \"702761\", \"001061\", \"300746\", \"102095\", \"304135\", \"100120\", \"105165\", \"501319\", \"500921\", \"305169\", \"200607\", \"108264\", \"111035\", \"107563\"]\n",
    "separator = \"\\t\"\n",
    "ner_model = None\n",
    "def instanciate_model():\n",
    "    ner_model = pipeline(task='ner', \n",
    "        model= 'saattrupdan/nbailab-base-ner-scandi', \n",
    "        aggregation_strategy='first')\n",
    "    return ner_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10165/10165 [11:24<00:00, 14.84it/s]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(cache_path):\n",
    "    ner_model = instanciate_model()\n",
    "    conll_splits = []\n",
    "    for split in [\"train\", \"dev\"]:\n",
    "        with open(os.path.join(conll_folder, split+\".conll\"), encoding=\"UTF-8\") as rf:\n",
    "            conll_splits.append(rf.read().strip())\n",
    "    conll_sents = \"\\n\\n\".join(conll_splits).split(\"\\n\\n\")\n",
    "    sents = [] # dict with sent_id as key, dict with various data as value\n",
    "    for sent in tqdm(conll_sents):\n",
    "        #Extract sentence_id: #sent_id=201911-02-01\n",
    "        lines = sent.split(\"\\n\")\n",
    "        assert lines[0].startswith(\"#sent_id\") and separator not in lines[0]\n",
    "        assert all([separator in l for l in lines[1:]])\n",
    "        sent_id = lines.pop(0).split(\"=\")[1]\n",
    "        sent_data = {\"sent_id\": sent_id,\n",
    "                    \"doc_id\" : sent_id.split(\"-\")[0], \n",
    "                    \"conll_text\": \"\\n\".join(lines)}\n",
    "        sent_data[\"tokens\"], sent_data[\"tsa_tags\"] = conn_tolist(sent_data[\"conll_text\"], sep=separator)\n",
    "        sent_data[\"text\"] = \" \".join(sent_data[\"tokens\"])\n",
    "        sent_data[\"ners\"] = pred_ranges(ner_model(sent_data[\"text\"]))\n",
    "        sent_data[\"split\"] = \"test\" if sent_data[\"doc_id\"] in elsa_test else \"train\"\n",
    "        sents.append(sent_data)\n",
    "    with open (cache_path, \"w\", encoding = \"utf-8\") as wf:\n",
    "        json.dump(sents, wf, ensure_ascii=False )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the NE predictions to conll files\n",
    "- Convert the character-based boundaries from the NE pipeline to one tag per token\n",
    "- Join the TSA and NER tags: If overlap, add polarity to NER tag. If not, discard entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_tag(sent):\n",
    "    \"\"\"Receives one sent dict, returns a list of the NE tags for the sentence, based on all the NE raw data for each sentence from the NER pipeline earlier\n",
    "       Filtering for PER and ORG happens here\n",
    "    \"\"\"\n",
    "    ners = [ne for ne in sent[\"ners\"] if ne[\"tag\"] in [\"PER\", \"ORG\"]]\n",
    "    ne_tags = [\"O\"]* len(sent[\"tokens\"])\n",
    "    for ne in ners:\n",
    "        ongoing = False\n",
    "        token_start = 0\n",
    "        for idx, token in enumerate(sent[\"tokens\"]):\n",
    "            token_end = token_start+len(token)\n",
    "            if token_start in range(ne[\"start\"],ne[\"end\"]) or token_end in range(ne[\"start\"],ne[\"end\"]) :\n",
    "                # any overlap. The NER pipeline can re-tokenize words like Borten-regjeringen\n",
    "                if ongoing:\n",
    "                    first = \"I\"\n",
    "                else:\n",
    "                    first = \"B\"\n",
    "                ongoing = True\n",
    "                ne_tags[idx] = first+\"-\"+ne[\"tag\"]\n",
    "            else:\n",
    "                ongoing = False # Not really needed ince we have a separate run for each NE, but still\n",
    "\n",
    "            # print(sent[\"text\"][token_start:token_end], token)\n",
    "            token_start = token_end +1\n",
    "    return ne_tags\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10165 1345 8820\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(cache_path, encoding = \"utf-8\") as rf:\n",
    "    sents = json.load(rf)\n",
    "print(len(sents),len([s for s in sents if s[\"split\"] == \"test\"]),len([s for s in sents if s[\"split\"] == \"train\"]))\n",
    "for idx, sent in enumerate(sents.copy()):\n",
    "    sent[\"ne_tags\"] = ner_tag(sent)\n",
    "    ne_tagspans = tag_span(sent[\"ne_tags\"])\n",
    "    tsa_tagspans = tag_span(sent[\"tsa_tags\"])\n",
    "    sent[\"elsa_tags\"] = spans_to_list([\"O\"]*len(sent[\"ne_tags\"]), merge_tags(ne_tagspans,tsa_tagspans ))\n",
    "    sent[\"elsapol_tags\"] = compresstags(sent[\"elsa_tags\"])\n",
    "    sents[idx] = sent\n",
    "\n",
    "with open (cache_path, \"w\", encoding = \"utf-8\") as wf:\n",
    "    json.dump(sents, wf, ensure_ascii=False )\n",
    "\n",
    "# Write elsa conll\n",
    "conll_sents ={\"train\": [], \"test\":[] }# list of sentence conll texts\n",
    "separator = \"\\t\"\n",
    "for sent in sents:\n",
    "    sent_lines = [\"#sent_id=\"+sent[\"sent_id\"]]\n",
    "    for token, tag in zip(sent[\"tokens\"], sent[\"elsa_tags\"]):\n",
    "        sent_lines.append(token+separator+tag)\n",
    "    \n",
    "    conll_sents[sent[\"split\"]].append(\"\\n\".join(sent_lines))\n",
    "\n",
    "if not os.path.exists(elsa_target_folder):\n",
    "    os.mkdir(elsa_target_folder)\n",
    "for split, c_sents in conll_sents.items():\n",
    "    path = os.path.join(elsa_target_folder, split+\".conll\")\n",
    "    with open(path, \"w\", encoding = \"utf-8\") as wf:\n",
    "        wf.write(\"\\n\\n\".join(c_sents))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert ELSA data to hf datasets\n",
    "- Create conversion table between tags and integers\n",
    "- Create pd.DataFrame with id, tokens and elsa_tags for each split\n",
    "- Create dataset, update info / features.\n",
    "- Create and save datasetdict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8820\n",
      "1345\n",
      "train tsa_tags 8820\n",
      "train ne_tags 8820\n",
      "train elsa_tags 8820\n",
      "train elsapol_tags 8820\n",
      "test tsa_tags 1345\n",
      "test ne_tags 1345\n",
      "test elsa_tags 1345\n",
      "test elsapol_tags 1345\n"
     ]
    }
   ],
   "source": [
    "sents[900][\"sent_id\"], sents[900][\"tokens\"], sents[900][\"elsa_tags\"]\n",
    "keysets = {key:set()for key in [\"tsa_tags\", \"ne_tags\", \"elsa_tags\", \"elsapol_tags\"]}\n",
    "keylists =  {key:[]for key in keysets}\n",
    "\n",
    "\n",
    "for key in keysets:\n",
    "    for sent in sents:\n",
    "        keysets[key].update(sent[key])\n",
    "    # print(key,keysets[key])\n",
    "    keylist = list(keysets[key])\n",
    "    keylist.remove(\"O\")\n",
    "    keylist.sort(key=lambda x: x[::-1])\n",
    "    keylists[key] = [\"O\"]+keylist\n",
    "label_to_ids = {key:{l:i for i, l in enumerate(keylist)} for key, keylist in keylists.items()} # Each text label gets their index position\n",
    "\n",
    "sents_split = {split:[]for split in [\"train\", \"test\"]}\n",
    "df_splits = {split:pd.DataFrame() for split in sents_split}\n",
    "for sent in sents:\n",
    "    sents_split[sent[\"split\"]].append(sent)\n",
    "[print(len(l)) for l in sents_split.values()]\n",
    "for split, s_sents in sents_split.items():\n",
    "    df_splits[split][\"id\"] = [el[\"sent_id\"] for el in s_sents]\n",
    "    df_splits[split][\"tokens\"] = [el[\"tokens\"] for el in s_sents]\n",
    "    for col in keysets:\n",
    "        new_col =  []\n",
    "        for s_sent in s_sents:\n",
    "            new_col.append([label_to_ids[col][l] for l in s_sent[col]])\n",
    "        print(split, col,len(new_col))\n",
    "        df_splits[split][col] = new_col\n",
    "\n",
    "# df_splits[\"train\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train tsa_tags [('O', 138113), ('B-targ-Positive', 3603), ('I-targ-Positive', 3541), ('B-targ-Negative', 1608), ('I-targ-Negative', 1535)]\n",
      "train ne_tags [('O', 142116), ('B-PER', 3109), ('I-PER', 1851), ('B-ORG', 882), ('I-ORG', 442)]\n",
      "train elsa_tags [('O', 142116), ('B-PER-Neutral', 2380), ('I-PER-Neutral', 1400), ('B-ORG-Neutral', 669), ('B-PER-Positive', 565), ('I-PER-Positive', 361), ('I-ORG-Neutral', 321), ('B-PER-Negative', 164), ('B-ORG-Positive', 152), ('I-PER-Negative', 90), ('I-ORG-Positive', 89), ('B-ORG-Negative', 61), ('I-ORG-Negative', 32)]\n",
      "train elsapol_tags [('O', 142116), ('B-Neutral', 3049), ('I-Neutral', 1721), ('B-Positive', 717), ('I-Positive', 450), ('B-Negative', 225), ('I-Negative', 122)]\n",
      "test tsa_tags [('O', 20418), ('B-targ-Positive', 504), ('I-targ-Positive', 442), ('B-targ-Negative', 206), ('I-targ-Negative', 181)]\n",
      "test ne_tags [('O', 20862), ('B-PER', 494), ('I-PER', 296), ('B-ORG', 75), ('I-ORG', 24)]\n",
      "test elsa_tags [('O', 20862), ('B-PER-Neutral', 381), ('I-PER-Neutral', 222), ('B-PER-Positive', 90), ('I-PER-Positive', 62), ('B-ORG-Neutral', 61), ('B-PER-Negative', 23), ('I-ORG-Neutral', 18), ('I-PER-Negative', 12), ('B-ORG-Positive', 9), ('B-ORG-Negative', 5), ('I-ORG-Positive', 3), ('I-ORG-Negative', 3)]\n",
      "test elsapol_tags [('O', 20862), ('B-Neutral', 442), ('I-Neutral', 240), ('B-Positive', 99), ('I-Positive', 65), ('B-Negative', 28), ('I-Negative', 15)]\n"
     ]
    }
   ],
   "source": [
    "# Count the various tags\n",
    "for split in [\"train\", \"test\"]:\n",
    "    for key in [\"tsa_tags\", \"ne_tags\", \"elsa_tags\", \"elsapol_tags\"]:\n",
    "        tags = [t for sent in sents for t in sent[key] if sent[\"split\"] == split]\n",
    "        print(split, key, Counter(tags).most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build new features dict\n",
    "new_features ={'id': Value(dtype='string', id=None), 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}\n",
    "for feat, f_names in keylists.items():\n",
    "    new_features[feat] = Sequence(feature=ClassLabel(num_classes = len(f_names), names = f_names, id=None), length=-1, id=None)\n",
    "\n",
    "# , 'pos_tags': Sequence(feature=ClassLabel(num_classes=47, names=['\"', \"''\", '#', '$', '(', ')', ',', '.', ':', '``', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'NN|SYM', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB'], id=None), length=-1, id=None), 'chunk_tags': Sequence(feature=ClassLabel(num_classes=23, names=['O', 'B-ADJP', 'I-ADJP', 'B-ADVP', 'I-ADVP', 'B-CONJP', 'I-CONJP', 'B-INTJ', 'I-INTJ', 'B-LST', 'I-LST', 'B-NP', 'I-NP', 'B-PP', 'I-PP', 'B-PRT', 'I-PRT', 'B-SBAR', 'I-SBAR', 'B-UCP', 'I-UCP', 'B-VP', 'I-VP'], id=None), length=-1, id=None), 'ner_tags': Sequence(feature=ClassLabel(num_classes=9, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'tsa_tags', 'ne_tags', 'elsa_tags', 'elsapol_tags'],\n",
       "        num_rows: 8820\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'tsa_tags', 'ne_tags', 'elsa_tags', 'elsapol_tags'],\n",
       "        num_rows: 1345\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'tsa_tags', 'ne_tags', 'elsa_tags', 'elsapol_tags'],\n",
       "        num_rows: 1345\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Dataset\n",
    "elsa_dses = {\"train\":None, \"test\":None}\n",
    "for split in elsa_dses:\n",
    "    elsa_dses[split] = datasets.Dataset.from_pandas(df_splits[split])\n",
    "    elsa_dses[split].features.update(new_features)\n",
    "    # print(elsa_dses[split].features)\n",
    "elsa_dses[\"validation\"] = datasets.Dataset.from_pandas(df_splits[\"test\"])\n",
    "dsd = datasets.DatasetDict(elsa_dses)\n",
    "dsd.save_to_disk(elsa_ds_folder)\n",
    "dsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Etter O O\n",
      "filmer O O\n",
      "som O O\n",
      "« O O\n",
      "Sideways O O\n",
      "» O O\n",
      "og O O\n",
      "« O O\n",
      "About O O\n",
      "Schmidt O O\n",
      "» O O\n",
      "må O O\n",
      "vel O O\n",
      "regissør B-targ-Positive O\n",
      "Alexander I-targ-Positive B-PER\n",
      "Payne I-targ-Positive I-PER\n",
      "sies O O\n",
      "å O O\n",
      "være O O\n",
      "en O O\n",
      "slags O O\n",
      "ekspert O O\n",
      "på O O\n",
      "dette O O\n",
      ". O O\n",
      "\n",
      "Julenissen , Ole Lukkøye , Tannfeen og Påskeharen danner et fellesskap som voktere av verdens barn , men er avhengig av at barna tror på deres eksistens .\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['B-PER', 'O', 'B-PER', 'I-PER', 'O', 'B-PER', 'O', 'B-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['B-PER-Neutral', 'O', 'B-PER-Neutral', 'I-PER-Neutral', 'O', 'B-PER-Neutral', 'O', 'B-PER-Neutral', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "En hyllest til Marit Bjørgen er det også blitt plass til .\n",
      "['O', 'O', 'O', 'B-targ-Positive', 'I-targ-Positive', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'B-PER-Positive', 'I-PER-Positive', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Det heile framstår så lite koherent at ein tek seg sjølv i å lengta tilbake til tida der Casablancas styrte skuta nærmast eigenhendig .\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG-Neutral', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "demo_ner = {'sent_id': '102727-04-02', 'doc_id': '102727', 'conll_text': 'Etter\\tO\\nfilmer\\tO\\nsom\\tO\\n«\\tO\\nSideways\\tO\\n»\\tO\\nog\\tO\\n«\\tO\\nAbout\\tO\\nSchmidt\\tO\\n»\\tO\\nmå\\tO\\nvel\\tO\\nregissør\\tB-targ-Positive\\nAlexander\\tI-targ-Positive\\nPayne\\tI-targ-Positive\\nsies\\tO\\nå\\tO\\nvære\\tO\\nen\\tO\\nslags\\tO\\nekspert\\tO\\npå\\tO\\ndette\\tO\\n.\\tO', 'tokens': ['Etter', 'filmer', 'som', '«', 'Sideways', '»', 'og', '«', 'About', 'Schmidt', '»', 'må', 'vel', 'regissør', 'Alexander', 'Payne', 'sies', 'å', 'være', 'en', 'slags', 'ekspert', 'på', 'dette', '.'], 'tsa_tags': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-targ-Positive', 'I-targ-Positive', 'I-targ-Positive', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'text': 'Etter filmer som « Sideways » og « About Schmidt » må vel regissør Alexander Payne sies å være en slags ekspert på dette .', 'ners': [{'start': 19, 'end': 27, 'tag': 'MISC', 'text': 'Sideways'}, {'start': 35, 'end': 48, 'tag': 'MISC', 'text': 'About Schmidt'}, {'start': 67, 'end': 82, 'tag': 'PER', 'text': 'Alexander Payne'}], 'split': 'train'}\n",
    "# print(demo_ner[\"text\"][67:82])\n",
    "ne_tags = ner_tag(demo_ner)\n",
    "for token, tsa, ner in zip(demo_ner[\"tokens\"], demo_ner[\"tsa_tags\"], ne_tags):\n",
    "    print(token, tsa, ner)\n",
    "\n",
    "n = 0\n",
    "while n < 3:\n",
    "    sent = random.choice(sents)\n",
    "    if not all([t == \"O\" for t in sent[\"elsa_tags\"]]):\n",
    "        n += 1\n",
    "        print()\n",
    "        for key in [\"text\", \"tsa_tags\", \"ne_tags\", \"elsa_tags\"]:\n",
    "            print(sent[key])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('transform')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63f766e18cf02043d406c2f113693a415f1494f09983f05ef5cfd3ee3ed0acbc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
