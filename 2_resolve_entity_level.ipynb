{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resolve the data to entity-level\n",
    "The data_cache now has the information we need, on the sentence level. We resolve the entities for each document, and record the sentiment scores for the document and sentence(s) they occur in. We also resolve any sentiment directed towards them at the target level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# venv transform\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "from helpers import *\n",
    "from transformers import  pipeline\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "save_root = \"outputs/\"\n",
    "tabular_savefolder = os.path.join(save_root, \"tabular\")\n",
    "os.makedirs(tabular_savefolder, exist_ok=True)\n",
    "data_cache = os.path.join(tabular_savefolder, \"data_sentencewise.json\")\n",
    "with open(data_cache, encoding = \"utf-8\") as rf:\n",
    "    dataset = json.load(rf)\n",
    "inspect_ids = tuple(set([s[\"doc_id\"] for s in dataset]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resolve entities to document-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_sent_pols(polarities):\n",
    "    \"\"\"List of sentence polarities the entity appears in.\n",
    "    Most commom pos or neg wins. Mixed for tie\"\"\"\n",
    "    strengths = {}\n",
    "    for polarity in ['Positive', 'Negative', 'Neutral']:\n",
    "        strengths[polarity] = len([p for p in polarities if p == polarity])\n",
    "        if strengths[polarity] == len(polarities):\n",
    "            return polarity\n",
    "    if strengths[\"Positive\"] > strengths[\"Negative\"]:\n",
    "        return \"Positive\"\n",
    "    if strengths[\"Negative\"] > strengths[\"Positive\"]:\n",
    "        return[\"Negative\"]\n",
    "    return \"Mixed\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PER ['Positive', 'Negative', 'Neutral', 'Neutral', 'Negative', 'Neutral', 'Positive', 'Neutral', 'Neutral']\n",
      "Mixed\n",
      "['tokens', 'tags', 'header', 'sent_id', 'doc_id', 'text', 'tsa_details', 'ner_firsts', 'doc_rating', 'doc_category', 'sentence_pol', 'nes']\n"
     ]
    }
   ],
   "source": [
    "doc_dfs = [] # List of document-entity dataframes to simplify the merging\n",
    "for doc_id in inspect_ids:\n",
    "    doc_ents = []\n",
    "    doc_sents = [s for s in dataset if s[\"doc_id\"] == doc_id]\n",
    "    ne_groups = [] # List of lists of groups\n",
    "    for sent in doc_sents:\n",
    "        for named_e in sent[\"nes\"]:\n",
    "            # Collect entity with substring match and keep enough data for later\n",
    "            ne = named_e[\"text\"]\n",
    "            found = False\n",
    "            for ne_group in ne_groups:\n",
    "                if any([ne in existing for existing in ne_group]) or any([existing in ne for existing in ne_group]):\n",
    "                    ne_group.append(ne)\n",
    "                    found = True\n",
    "                    break\n",
    "                else :\n",
    "                    stripped = ne.rstrip(\"s\").rstrip(\"'\").rstrip()\n",
    "                    if any([stripped in existing for existing in ne_group]):\n",
    "                        ne_group.append(ne)\n",
    "                        found = True\n",
    "                        break\n",
    "\n",
    "            if not found:\n",
    "                ne_groups.append([ne])\n",
    "        \n",
    "    # Now, each named entity document-level is a list of apperances in the text.\n",
    "    # Next step is to iterate these and make a dataframe\n",
    "    doc_entities = {} # Resolved name as key, all mentions in value\n",
    "    for s_forms in ne_groups:\n",
    "        # e_ent is a list of dicts representing each surface form of same entity\n",
    "        # Find longest text representat\n",
    "        longest = max(s_forms, key=len)\n",
    "        if longest.rstrip(\"s\").rstrip(\"'\").rstrip() in s_forms:\n",
    "            longest = longest.rstrip(\"s\").rstrip(\"'\").rstrip()\n",
    "        doc_entities[longest] = list(set(s_forms))\n",
    "\n",
    "        \n",
    "    # If we read the entities like [\"John\", \"Wayne\",  \"John Wayne\"] We get two lists because Wayne is not substring og John. Fixing this\n",
    "    for one, two in product(doc_entities.copy(), doc_entities.copy()):\n",
    "        if not one == two and one in two:\n",
    "            doc_entities[two] += doc_entities[one]\n",
    "            del doc_entities[one]\n",
    "    \n",
    "    # print(doc_id, doc_entities)\n",
    "\n",
    "    \"Double check no duplicate entries in different surface form lists\"\n",
    "    all_surface_forms = [f for  s_forms in doc_entities.values() for f in s_forms]\n",
    "    assert len(all_surface_forms) == len(set(all_surface_forms))\n",
    "\n",
    "    # Populate the entity with more data\n",
    "    for longest, s_forms in doc_entities.items():\n",
    "        doc_entities[longest] = {\"surface_forms\": s_forms,\n",
    "            \"doc_id\": doc_id,\n",
    "            \"entity_id\":  doc_id+\"_\"+\"_\".join(longest.split())\n",
    "        }\n",
    "\n",
    "    for longest, ent_data in doc_entities.copy().items():\n",
    "        sents_having = []\n",
    "        nes_belonging = []\n",
    "        for sent in doc_sents:\n",
    "            if any ([ne[\"text\"] in ent_data[\"surface_forms\"] for ne in sent[\"nes\"]]):\n",
    "                sents_having.append(sent)\n",
    "            nes_belonging += [ne for ne in sent[\"nes\"] if ne[\"text\"] in ent_data[\"surface_forms\"]]\n",
    "        assert len(sents_having) > 0\n",
    "        assert len((nes_belonging)) >= len(ent_data[\"surface_forms\"])\n",
    "\n",
    "        doc_entities[\"ne_cat\"] = Counter([ne[\"tag\"] for ne in nes_belonging]).most_common(1)[0][0]\n",
    "        doc_entities[\"sentences_pol\"] = [s[\"sentence_pol\"] for s in sents_having]\n",
    "        doc_entities[\"sent_pol_resolved\"] = resolve_sent_pols(doc_entities[\"sentences_pol\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dont = [\"header\", \"tokens\", \"doc_id\"]\n",
    "has_something = [s for s in doc_sents if len(s[\"tsa_details\"]) > 1 and len(s[\"ner_firsts\"]) > 0 ]\n",
    "has_something = [s for s in doc_sents if len(s[\"nes\"]) > 1  ]\n",
    "# [print (key, value) for key, value in has_something[1].items() if key not in dont]\n",
    "print(doc_entities[\"ne_cat\"], doc_entities[\"sentences_pol\"])\n",
    "print(doc_entities[\"sent_pol_resolved\"])\n",
    "print(list(sent))\n",
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('transform')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63f766e18cf02043d406c2f113693a415f1494f09983f05ef5cfd3ee3ed0acbc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
